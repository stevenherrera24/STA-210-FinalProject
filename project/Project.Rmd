---
title: "What Makes the Best Tennis Player?"
author: "Steven Herrera and Ethan Shen"
date: "11/09/2018"
output:
  pdf_document: default
  html_document: default
---

```{r setup, message=F,include=F}
knitr::opts_chunk$set(echo = TRUE)
options(knitr.kable.NA = '')
```

# Loaded Packages

```{r load-packages, message=FALSE}
library(tidyverse)
library(olsrr)
library(cowplot)
library(car)
library(broom)
library(knitr)
library(arm)
library(tidyr)
library(pROC)
library(arm)
library(rlm)
```

# Data Manipulation

```{r message=FALSE}
atp <- read_csv("files/atp.csv")
atp2016 <- read_csv("files/atp2016.csv")
atp2015 <- read_csv("files/atp2015.csv")
atp2014 <- read_csv("files/atp2014.csv")
atp2013 <- read_csv("files/atp2013.csv")
atp2012 <- read_csv("files/atp2012.csv")
atp2011 <- read_csv("files/atp2011.csv")
atp2010 <- read_csv("files/atp2010.csv")

atp1 <- atp %>%
  filter(tourney_date < 20171113)

winners2016 <- atp2016 %>%
  filter(tourney_date < 20161114)

winners2015 <- atp2015 %>%
  filter(tourney_date < 20151115) 

winners2014 <- atp2014 %>%
  filter(tourney_date < 20141109) 

winners2013 <- atp2013 %>%
  filter(tourney_date < 20131104) 

winners2012 <- atp2012 %>%
  filter(tourney_date < 20121105) 

winners2011 <- atp2011 %>%
  filter(tourney_date < 20111114) 

winners2010 <- atp2010 %>%
  filter(tourney_date < 20101121) 

winners <- rbind(atp1, winners2016, winners2015, winners2014, winners2013, 
                 winners2012, winners2011, winners2010)

winners_set <- winners %>%
  filter(best_of == 3,
         !is.na(w_ace), 
         !is.na(w_df), 
         !is.na(w_svpt), 
         !is.na(w_1stIn), 
         !is.na(w_1stWon), 
         !is.na(w_2ndWon),
         !is.na(w_SvGms), 
         !is.na(w_bpSaved),
         !is.na(w_bpFaced),
         surface!="None")

losers <- winners_set %>%
  mutate(seed = loser_seed,
         name = loser_name,
         hand = loser_hand,
         ht = loser_ht,
         age = loser_age,
         rank = loser_rank,
         rankpoints = loser_rank_points,
         ace = l_ace,
         df = l_df,
         svpt = l_svpt,
         firsstIn = l_1stIn,
         firsttWon = l_1stWon,
         secondndWon = l_2ndWon,
         SvGms = l_SvGms,
         bpSaved = l_bpSaved,
         bpFaced = l_bpFaced,
         minutes = minutes,
         status = 0
         )  
  

winners <- winners_set %>%
  mutate(seed = winner_seed,
         name = winner_name,
         hand = winner_hand,
         ht = winner_ht,
         age = winner_age,
         rank = winner_rank,
         rankpoints = winner_rank_points,
         ace = w_ace,
         df = w_df,
         svpt = w_svpt,
         firsstIn = w_1stIn,
         firsttWon = w_1stWon,
         secondndWon = w_2ndWon,
         SvGms = w_SvGms,
         bpSaved = w_bpSaved,
         bpFaced = w_bpFaced,
         minutes = minutes,
         status = 1
         ) 
tennis <- rbind(winners,losers)

tennis <- tennis %>%
  filter(!is.na(seed),
         !is.na(name),
         !is.na(hand),
         !is.na(ht),
         !is.na(age),
         !is.na(rank),
         !is.na(rankpoints),
         !is.na(ace),
         !is.na(df),
         !is.na(svpt),
         !is.na(firsstIn),
         !is.na(firsttWon),
         !is.na(secondndWon),
         !is.na(SvGms),
         !is.na(bpSaved),
         !is.na(bpFaced),
         !is.na(minutes),
         !is.na(status))
```

# Data 

Using data manipulation skills in R, we shaped the dataset to show each observation as the outcome of the match for the winner and the loser of each match from 2010-2017. Below, is a glimpse of our dataset.

```{r}
glimpse(tennis)
```

Because we have 11,037 observations, we will randomly select 1000 observations to be included in a smaller dataset so that we can effectively examine exploratory data analysis. 

```{r}
set.seed(1234)
ten <- tennis %>% sample_n(1000)
```

Here is what our new dataset looks like:

```{r}
glimpse(ten)
```

# Exploratory Data Analysis 

To begin our exploratory data analysis, we will examine a matrix plot of the variables in our dataset to consider multicollinearity and large leverage of certain observations.

## Matrix Plot
```{r fig.height=11, fig.width=11}
ten <- ten %>%
  mutate(status = as.factor(status)) 

pairs(status ~ minutes + ht + age + rank + rankpoints + ace +
        df + svpt + firsstIn + firsttWon + secondndWon + 
        SvGms + bpSaved + bpFaced, data=ten, pch = 16,
      main = "Matrix of scatterplots for Tournament Wins and Variables")
```

Looking at the matrix plot, we will consider removing the following variables because of multicollinearity: `svpt`, `firsstIn`, `firsttWon`, `secondndWon`, `SvGms`, and `bpFaced`.

We will now look at the box plots of the numeric variables that we will include in our full model:

```{r fig.height=10, fig.width=8, message=FALSE}
p1 <- ggplot(data=ten,aes(x=status,y=minutes, group=status)) +
  geom_boxplot() + 
  labs(title="Minutes by Status",
       x = "0 if Lost, 1 if Won",
       y = "Minutes")
p2 <- ggplot(data=ten,aes(x=status,y=ht, group=status)) +
  geom_boxplot() + 
  labs(title="Height by Status",
       x = "0 if Lost, 1 if Won",
       y = "Height")
p3 <- ggplot(data=ten,aes(x=status,y=age, group=status)) +
  geom_boxplot() + 
  labs(title="Age by Status",
       x = "0 if Lost, 1 if Won",
       y = "Age (years)")
p4 <- ggplot(data=ten,aes(x=status,y=rank, group=status)) +
  geom_boxplot() + 
  labs(title="Ranking by Status",
       x = "0 if Lost, 1 if Won",
       y = "Rank")
p5 <- ggplot(data=ten,aes(x=status,y=rankpoints, group=status)) +
  geom_boxplot() + 
  labs(title="Rankpoints by Status",
       x = "0 if Lost, 1 if Won",
       y = "Rankpoints")
p6 <- ggplot(data=ten,aes(x=status,y=ace, group=status)) +
  geom_boxplot() + 
  labs(title="Aces by Status",
       x = "0 if Lost, 1 if Won",
       y = "Aces")
p7 <- ggplot(data=ten,aes(x=status,y=df, group=status)) +
  geom_boxplot() + 
  labs(title="Double Faults by Status",
       x = "0 if Lost, 1 if Won",
       y = "Double Faults")
p8 <- ggplot(data=ten,aes(x=status,y=bpSaved, group=status)) +
  geom_boxplot() + 
  labs(title="Saved Breakpoints by Status",
       x = "0 if Lost, 1 if Won",
       y = "Saved Breakpoints")

plot_grid(p1,p2,p3,p4,p5,
          p6,p7,p8,ncol=2)
```

And we will include a stacked bar graph for the variable `surface`.

```{r}
ggplot(data=ten,aes(x=surface, fill = status)) + geom_bar(position = "fill") + 
  labs(title="Status vs. Surface")
```

In looking at all of these observations, it seems like the medians of the numeric distributions do not seem to differ that much by status of winning or losing. The same can be said about the proportions of winning and losing matches against all three surfaces. In creating our model, it could be difficult to see which variables could be helpful in differentiating between whether a player will win a match or not. But, we hope to see that a combination of these variables will be helpful in determining a model that best predicts the percentage of winning a match.

# Logistic Regression Model: Version 1

To begin our regression models, we will use all of the variables we deemed important from our exploratory data analysis.

```{r}
full_model <- glm(status ~ minutes + ht + age + rank + 
                rankpoints + ace + df + bpSaved + surface, 
                family=binomial,data=ten)
kable(tidy(full_model), format="markdown", digits = 3)

```

```{r}
full_w_interactions <- glm(status ~ minutes + ht + age + rank + 
                rankpoints + ace + df + bpSaved + surface + surface * minutes +
                surface * ht + surface * age + surface * rank + surface * rankpoints + 
                surface * ace + surface * df + surface * bpSaved,
                family=binomial,data=ten)
```

```{r}
model.selected.interactions <- step(full_w_interactions,direction="backward")
```

# Model

Below, we have used the results from the backwards selection method and created a new model, `final.base.model`.

```{r}
final.base.model <- model.selected.interactions
kable(tidy(final.base.model), format = "markdown", digits = 3)
```

# Model Assessment

## Binned Residual Plots 

We will further our investigation of whether our new model follows the key model assessment characteristics:

- Good binned residual vs. predicted plot

- Good binned residual vs. numerical explanatory plots

- Large area under the ROC curve

```{r}
ten <- ten %>% mutate(Residuals = residuals.glm(final.base.model,type="response"),
                          Predicted = predict.glm(final.base.model,type="response"))

binnedplot(ten$Predicted, ten$Residuals,xlab="Predicted Probabilities",
           ylab="Residuals",main="Binned Residuals vs. Predicted Probabilities")
```

```{r,fig.height=4,fig.width=4,echo=F}
binnedplot(ten$minutes, ten$Residuals,xlab="Minutes",
           ylab="Residuals",main="Binned Residuals vs. Minutes")

binnedplot(ten$ht, ten$Residuals,xlab="Height",
           ylab="Residuals",main="Binned Residuals vs. Height")

binnedplot(ten$rankpoints, ten$Residuals,xlab="Rankpoints",
           ylab="Residuals",main="Binned Residuals vs. Rankpoints")

binnedplot(ten$ace, ten$Residuals,xlab="Aces",
           ylab="Residuals",main="Binned Residuals vs. Aces")

binnedplot(ten$df, ten$Residuals,xlab="Double Faults",
           ylab="Residuals",main="Binned Residuals vs. Double Faults")

binnedplot(ten$bpSaved, ten$Residuals,xlab="Saved break podints",
           ylab="Residuals",main="Binned Residuals vs. Saved Break Points")
```

```{r}
ROC.ten <- roc(ten$status,ten$Predicted,plot=T)
```

```{r}
ROC.ten$auc
```

Because our binned residual vs. `bpSaved` plot has a non-linear relationship, we decided to remove the variable.

# Influential Points 

## VIF

```{r}
tidy(vif(final.base.model))
```

After looking at the VIF values, we see that the VIF for `surface` is greater than 10, so we will also remove it from the model. This means we will also have to remove the interaction variables as well. Since there are issues with this model, we will not continue checking the other assumptions. 

# Linear Regression Assumptions: Version 2 

This is our revised final model, after removing `surface` and `bpSaved`. We will use this model to conduct predictions and our analysis. The assumptions are in a separate Rmd file. 

```{r}
newten <- ten
final <- glm(status ~ minutes + ht + rankpoints + ace + df, family = binomial, data = newten)
kable(tidy(final), format = "markdown", digits = 6)
```

```{r}
pairs(status ~ minutes + ht + rankpoints + ace + df, data = newten)
```

# Prediction

## Test cases: 

We will look at a match that was played at the Monte Carlo Masters in 2014 between Roger Federer and Novak Djokovic. We want to see the probability that either player will win the match. 

```{r message=FALSE}
tennis %>%
  filter(tourney_name == "Monte Carlo Masters",
         name == "Roger Federer" |  name == "Novak Djokovic",
         tourney_id == "2014-410",
         loser_name == "Novak Djokovic")
```

```{r}
fed <- data.frame(minutes = 75, ht = 185, rankpoints = 5355 , ace = 3 , df = 1)
djo1 <- data.frame(minutes = 75, ht = 188, rankpoints = 11680, ace = 2, df = 0)
predict(final, newdata=fed, type="response")
predict(final, newdata=djo1, type="response")
```

```{r}
predsfed <- predict(final, fed, type="response", se.fit=TRUE)
predsdjo1 <- predict(final, djo1, type="response", se.fit=TRUE)

predffed <- predsfed$fit # predicted
lowerfed <- predsfed$fit - (1.96*predsfed$se.fit) # lower bounds
upperfed <- predsfed$fit + (1.96*predsfed$se.fit) # lower bounds

predfdjo1 <- predsdjo1$fit # predicted
lowerdjo1 <- predsdjo1$fit - (1.96*predsdjo1$se.fit) # lower bounds
upperdjo1 <- predsdjo1$fit + (1.96*predsdjo1$se.fit) # upper bounds

c(predffed, lowerfed, upperfed)
c(predfdjo1, lowerdjo1, upperdjo1)
```

The model predicts that Roger Federer has a 79.37% chance of winning the match, and we are 95% confident that the probability of Federer winning the match is between 74.74% and 84.00%. The model also predicts that Novak Djokovic has a 92.52% chance of winning the match, and we are 95% confident that the probability of Djokovic winning the match is between 87.16% and 96.49%. 

Given the model, Djokovic won. This is mainly because Djokovic's rank points were almost double that of Federer's at the time. However, Federer won the match. In this case, we had an anomaly. 

### Test Case with Similar Rank Points 

Now, we'll look at a match where both players had similar rank points. The match we will look at is between Jo Wilfried Tsonga and Michael Llodra, and was played at the Queen's Club Championships in 2011. 

```{r}
ten %>%
  filter(winner_rank_points > 1000 & winner_rank_points < 2000 & loser_rank_points > 1000 & loser_rank_points < 2000,
         tourney_id == "2011-311",
         tourney_name == "Queen's Club",
         winner_name == "Jo Wilfried Tsonga"
         )
```

```{r}
jo <- data.frame(minutes = 23, ht = 188, rankpoints = 1480, ace = 2, df = 1)
llodra <- data.frame(minutes = 23, ht = 190, rankpoints = 1400, ace = 0, df = 0)
predict(final, newdata=jo, type="response")
predict(final, newdata=llodra, type="response")
```

```{r}
predsjo <- predict(final, jo, type="response", se.fit=TRUE)
predsllodra <- predict(final, llodra, type="response", se.fit=TRUE)

predfjo <- predsjo$fit # predicted
lowerjo <- predsjo$fit - (1.96*predsjo$se.fit) # lower bounds
upperjo <- predsjo$fit + (1.96*predsjo$se.fit) # upper bounds

predfllodra <- predsllodra$fit # predicted
lowerllodra <- predsllodra$fit - (1.96*predsllodra$se.fit) # lower bounds
upperllodra <- predsllodra$fit + (1.96*predsllodra$se.fit) # lower bounds

c(predfjo, lowerjo, upperjo)
c(predfllodra, lowerllodra, upperllodra)
```

The model predicts that Jo Wilfried Tsonga has a 71.81% of chance of winning the match, and we are 95% confident that the probability of Tsonga winning the match is between 64.57% and 79.04%. The model predicts that Michael Llodra has a 70.16% chance of winning the match, and we are 95% confident that the probability of Llodra winning the match is between 62.05% and 78.28%. 

The model predicts that Tsonga has a higher probability of winning the match, and he was the winner. The probabilities of winning are much closer because the two players have similar rank points, but are lower than that of the previous match we examined because the players' rank points are significantly lower than those of Nadal's and Djokovicâ€™s.

# Conclusion 

In general, if both players have similar statistics, the model does a good job of predicting which player will win the match based off who has the higher probability of winning. If the players have a certain statistic that is significantly different, especially rank points, the model will predict that the player with higher rank points has a higher probability of winning. 

This indicates one of the flaws of our model; one player having a higher probability of winning does not mean that player won, as we saw with the first prediction example. Federer and Djokovic are both exceptional players, and most matches between the two are toss-ups. The easiest way to improve our predictions and their corresponding confidence intervals would be to run the simulation multiple times with different seeds (which determines the random selection process for selecting the 1000 observations) and determine which model coefficients do the best at predicting the probability of winning the match.





