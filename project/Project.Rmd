---
title: "What Makes the Best Tennis Player?"
author: "Steven Herrera and Ethan Shen"
date: "11/09/2018"
output:
  html_document: default
  pdf_document: default
---

```{r setup, message=F,include=F}
knitr::opts_chunk$set(echo = TRUE)
options(knitr.kable.NA = '')
```

# Loaded Packages

```{r load-packages, message=FALSE}
library(tidyverse)
library(olsrr)
library(cowplot)
library(car)
library(broom)
library(knitr)
library(arm)
library(tidyr)
library(pROC)
library(arm)
library(rlm)
```

# Data Manipulation

```{r message=FALSE}
atp <- read_csv("files/atp.csv")
atp2016 <- read_csv("files/atp2016.csv")
atp2015 <- read_csv("files/atp2015.csv")
atp2014 <- read_csv("files/atp2014.csv")
atp2013 <- read_csv("files/atp2013.csv")
atp2012 <- read_csv("files/atp2012.csv")
atp2011 <- read_csv("files/atp2011.csv")
atp2010 <- read_csv("files/atp2010.csv")

atp1 <- atp %>%
  filter(tourney_date < 20171113)

winners2016 <- atp2016 %>%
  filter(tourney_date < 20161114)

winners2015 <- atp2015 %>%
  filter(tourney_date < 20151115) 

winners2014 <- atp2014 %>%
  filter(tourney_date < 20141109) 

winners2013 <- atp2013 %>%
  filter(tourney_date < 20131104) 

winners2012 <- atp2012 %>%
  filter(tourney_date < 20121105) 

winners2011 <- atp2011 %>%
  filter(tourney_date < 20111114) 

winners2010 <- atp2010 %>%
  filter(tourney_date < 20101121) 

winners <- rbind(atp1, winners2016, winners2015, winners2014, winners2013, 
                 winners2012, winners2011, winners2010)

winners_set <- winners %>%
  filter(best_of == 3,
         !is.na(w_ace), 
         !is.na(w_df), 
         !is.na(w_svpt), 
         !is.na(w_1stIn), 
         !is.na(w_1stWon), 
         !is.na(w_2ndWon),
         !is.na(w_SvGms), 
         !is.na(w_bpSaved),
         !is.na(w_bpFaced),
         surface!="None")

losers <- winners_set %>%
  mutate(seed = loser_seed,
         name = loser_name,
         hand = loser_hand,
         ht = loser_ht,
         age = loser_age,
         rank = loser_rank,
         rankpoints = loser_rank_points,
         ace = l_ace,
         df = l_df,
         svpt = l_svpt,
         firsstIn = l_1stIn,
         firsttWon = l_1stWon,
         secondndWon = l_2ndWon,
         SvGms = l_SvGms,
         bpSaved = l_bpSaved,
         bpFaced = l_bpFaced,
         minutes = minutes,
         status = 0
         )  
  

winners <- winners_set %>%
  mutate(seed = winner_seed,
         name = winner_name,
         hand = winner_hand,
         ht = winner_ht,
         age = winner_age,
         rank = winner_rank,
         rankpoints = winner_rank_points,
         ace = w_ace,
         df = w_df,
         svpt = w_svpt,
         firsstIn = w_1stIn,
         firsttWon = w_1stWon,
         secondndWon = w_2ndWon,
         SvGms = w_SvGms,
         bpSaved = w_bpSaved,
         bpFaced = w_bpFaced,
         minutes = minutes,
         status = 1
         ) 
tennis <- rbind(winners,losers)

tennis <- tennis %>%
  filter(!is.na(seed),
         !is.na(name),
         !is.na(hand),
         !is.na(ht),
         !is.na(age),
         !is.na(rank),
         !is.na(rankpoints),
         !is.na(ace),
         !is.na(df),
         !is.na(svpt),
         !is.na(firsstIn),
         !is.na(firsttWon),
         !is.na(secondndWon),
         !is.na(SvGms),
         !is.na(bpSaved),
         !is.na(bpFaced),
         !is.na(minutes),
         !is.na(status))
```

# Data 

Using data manipulation skills in R, we shaped the dataset to show each observation as the outcome of the match for the winner and the loser of each match from 2010-2017. Below, is a glimpse of our dataset.

```{r}
glimpse(tennis)
```

Because we have 11,037 observations, we will randomly select observations to be included in a smaller dataset so that we can effectively examine exploratory data analysis. Below, is our code on how we randomly selected the observations in our new dataset.

```{r}
set.seed(1234)
ten <- tennis %>% sample_n(1000)
```

Here is what our new dataset looks like:

```{r}
glimpse(ten)
```

# Exploratory Data Analysis 

To begin our exploratory data analysis, we will examine a matrix plot of the variables in our dataset to consider multicollinearity and large leverage of certain observations.

## Matrix Plot
```{r fig.height=11, fig.width=11}
ten <- ten %>%
  mutate(status = as.factor(status)) 

pairs(status ~ minutes + ht + age + rank + rankpoints + ace +
        df + svpt + firsstIn + firsttWon + secondndWon + 
        SvGms + bpSaved + bpFaced, data=ten, pch = 16,
      main = "Matrix of scatterplots for Tournament Wins and Variables")
```

Looking at the matrix plot, we will consider removing the following favirables because of multicollinearity: `svpt`, `firsstIn`, `firsttWon`, `secondndWon`, `SvGms`, and `bpFaced`.

We will now look at box plots of the numeric variables we will include in our full model:

```{r fig.height=10, fig.width=8, message=FALSE}
p1 <- ggplot(data=ten,aes(x=status,y=minutes, group=status)) +
  geom_boxplot() + 
  labs(title="Minutes by Status",
       x = "0 if Lost, 1 if Won",
       y = "Minutes")
p2 <- ggplot(data=ten,aes(x=status,y=ht, group=status)) +
  geom_boxplot() + 
  labs(title="Height by Status",
       x = "0 if Lost, 1 if Won",
       y = "Height")
p3 <- ggplot(data=ten,aes(x=status,y=age, group=status)) +
  geom_boxplot() + 
  labs(title="Age by Status",
       x = "0 if Lost, 1 if Won",
       y = "Age (years)")
p4 <- ggplot(data=ten,aes(x=status,y=rank, group=status)) +
  geom_boxplot() + 
  labs(title="Ranking by Status",
       x = "0 if Lost, 1 if Won",
       y = "Rank")
p5 <- ggplot(data=ten,aes(x=status,y=rankpoints, group=status)) +
  geom_boxplot() + 
  labs(title="Rankpoints by Status",
       x = "0 if Lost, 1 if Won",
       y = "Rankpoints")
p6 <- ggplot(data=ten,aes(x=status,y=ace, group=status)) +
  geom_boxplot() + 
  labs(title="Aces by Status",
       x = "0 if Lost, 1 if Won",
       y = "Aces")
p7 <- ggplot(data=ten,aes(x=status,y=df, group=status)) +
  geom_boxplot() + 
  labs(title="Double Faults by Status",
       x = "0 if Lost, 1 if Won",
       y = "Double Faults")
p8 <- ggplot(data=ten,aes(x=status,y=bpSaved, group=status)) +
  geom_boxplot() + 
  labs(title="Saved Breakpoints by Status",
       x = "0 if Lost, 1 if Won",
       y = "Saved Breakpoints")

plot_grid(p1,p2,p3,p4,p5,
          p6,p7,p8,ncol=2)
```

And we will include a stacked bar graph for the variable `surface`.

```{r}
ggplot(data=ten,aes(x=surface, fill = status)) + geom_bar(position = "fill") + 
  labs(title="Status vs. Surface")
```

In looking at all of these observations, it seems like the medians of the numeric distributions do not seem to differ that much by status of winning or losing. The same can be said about the proportions of winning and losing matches against all three surfaces. In creating our model, it could be difficult to see which variables could be helpful in differentiating between whether a player will win a match or not. But, we hope to see that a combination of these variables will be helpful in determining a model that best predicts the percentage of winning a match.

# Logistic Regression Model: Version 1

To begin our regression models, we will use all of the variables we deemed important from our exploratory data analysis.

```{r}
full_model <- glm(status ~ minutes + ht + age + rank + 
                rankpoints + ace + df + bpSaved + surface, 
                family=binomial,data=ten)
kable(tidy(full_model), format="markdown", digits = 3)

```

```{r}
full_w_interactions <- glm(status ~ minutes + ht + age + rank + 
                rankpoints + ace + df + bpSaved + surface + surface * minutes +
                surface * ht + surface * age + surface * rank + surface * rankpoints + 
                surface * ace + surface * df + surface * bpSaved,
                family=binomial,data=ten)
```

```{r}
model.selected.interactions <- step(full_w_interactions,direction="backward")
```

# Model

```{r}
final.base.model <- model.selected.interactions
kable(tidy(final.base.model), format = "markdown", digits = 3)
```

# Model Assessment

## Binned Residual Plots 

```{r}
ten <- ten %>% mutate(Residuals = residuals.glm(final.base.model,type="response"),
                          Predicted = predict.glm(final.base.model,type="response"))

binnedplot(ten$Predicted, ten$Residuals,xlab="Predicted Probabilities",
           ylab="Residuals",main="Binned Residuals vs. Predicted Probabilities")
```

```{r,fig.height=4,fig.width=4,echo=F}
binnedplot(ten$minutes, ten$Residuals,xlab="Minutes",
           ylab="Residuals",main="Binned Residuals vs. Minutes")

binnedplot(ten$ht, ten$Residuals,xlab="Height",
           ylab="Residuals",main="Binned Residuals vs. Height")

binnedplot(ten$rankpoints, ten$Residuals,xlab="Rankpoints",
           ylab="Residuals",main="Binned Residuals vs. Rankpoints")

binnedplot(ten$ace, ten$Residuals,xlab="Aces",
           ylab="Residuals",main="Binned Residuals vs. Aces")

binnedplot(ten$df, ten$Residuals,xlab="Double Faults",
           ylab="Residuals",main="Binned Residuals vs. Double Faults")

binnedplot(ten$bpSaved, ten$Residuals,xlab="Saved break podints",
           ylab="Residuals",main="Binned Residuals vs. Saved Break Points")
```

```{r}
ROC.ten <- roc(ten$status,ten$Predicted,plot=T)
```

```{r}
ROC.ten$auc
```

```{r}
threshold = 0.30
table(ten$status, ten$Predicted > threshold)
```

```{r}
(326 + 13)/(26+13+326+635)
```
# Influential Points 

## VIF

```{r}
tidy(vif(final.base.model))
```

After looking at the VIF values, we see that the VIF for `surface` is greater than 10, so we will also remove it from the model. This means we will also have to remove the interaction variables as well. Since there are issues with this model, we will not continue checking the other assumptions. 

# Linear Regression Assumptions: Version 2 

Because one of our residuals plots has a non-linear relationship, we will remove `bpSaved` from the model and redo the assumptions. We will also remove `surface` and its corresponding interactions effects because `surface` has a high VIF value. 

```{r}
newten <- ten
final <- glm(status ~ minutes + ht + rankpoints + ace + df, family = binomial, data = newten)
kable(tidy(final), format = "markdown", digits = 3)
```

```{r}
pairs(status ~ minutes + ht + rankpoints + ace + df, data = newten)
```

The scatterplot matrix does not show obvious signs of multicollinearity. 

## Model Assessment

### Binned Plots with Residuals vs Predicted

```{r}
newten <- newten %>% mutate(Residuals = residuals.glm(final,type="response"),
                          Predicted = predict.glm(final,type="response"))

binnedplot(newten$Predicted, newten$Residuals,xlab="Predicted Probabilities",
           ylab="Residuals",main="Binned Residuals vs. Predicted Probabilities")
```

```{r,fig.height=4,fig.width=4,echo=F}
binnedplot(newten$minutes, newten$Residuals,xlab="Minutes",
           ylab="Residuals",main="Binned Residuals vs. Minutes")

binnedplot(newten$ht, newten$Residuals,xlab="Height",
           ylab="Residuals",main="Binned Residuals vs. Height")

binnedplot(newten$ace, newten$Residuals,xlab="Aces",
           ylab="Residuals",main="Binned Residuals vs. Aces")

binnedplot(newten$df, newten$Residuals,xlab="Double Faults",
           ylab="Residuals",main="Binned Residuals vs. Double Faults")

binnedplot(newten$rankpoints, newten$Residuals,xlab="Rankpoints",
           ylab="Residuals",main="Binned Residuals vs. Rankpoints")
```

### ROC curve and Confusion Matrix 

```{r}
ROC.newten <- roc(newten$status,newten$Predicted,plot=T)
```

```{r}
ROC.newten$auc
```

```{r}
threshold = 0.30
table(newten$status, newten$Predicted > threshold)
```

```{r}
(342 + 10)/(342 + 10 + 10 + 638)
```

## Influential Points 

```{r}
newten <- newten %>%
  mutate(leverage = hatvalues(final), 
         cooks = cooks.distance(final),
         stand.resid = rstandard(final), 
         obs.num = row_number())
```

### Leverage and Cook's Distance 

```{r}
p1 <- ggplot(data=newten, aes(x=obs.num,y=leverage)) + 
  geom_point(alpha=0.5) + 
  geom_hline(yintercept=0.1,color="red")+
  labs(x="Observation Number",y="Leverage",title="Leverage")

p2 <- ggplot(data=newten, aes(x=obs.num,y=cooks)) + 
  geom_point() + 
  geom_hline(yintercept=1,color="red")+
  labs(x="Observation Number",y="Cook's Distance",title="Cook's Distance")

plot_grid(p1,p2)
```

## VIF 

```{r}
tidy(vif(final))
```

There are no observations with large leverage or a Cook's Distance that is above the threshold. We also see that the VIF values of our variables are all less than 10. 

# Prediction

## Test cases: 

We will look at a match that was played the Monte Carlo Masters in 2014 between Roger Federer and Novak Djokovic. We want to see the probability that a player will win the match. 

```{r message=FALSE}
tennis %>%
  filter(tourney_name == "Monte Carlo Masters",
         name == "Roger Federer" |  name == "Novak Djokovic",
         tourney_id == "2014-410",
         loser_name == "Novak Djokovic")
```

```{r}
fed <- data.frame(minutes = 75, ht = 185, rankpoints = 5355 , ace = 3 , df = 1)
djo1 <- data.frame(minutes = 75, ht = 188, rankpoints = 11680, ace = 2, df = 0)
predict(final, newdata=fed, type="response")
predict(final, newdata=djo1, type="response")
```

```{r}
predsdjo1 <- predict(final, djo1, type="response", se.fit=TRUE)
predsfed <- predict(final, fed, type="response", se.fit=TRUE)

predfdjo1 <- predsdjo1$fit # predicted
lowerdjo1 <- predsdjo1$fit - (1.96*predsdjo1$se.fit) # lower bounds
upperdjo1 <- predsdjo1$fit + (1.96*predsdjo1$se.fit) # upper bounds

predffed <- predsfed$fit # predicted
lowerfed <- predsfed$fit - (1.96*predsfed$se.fit) # lower bounds
upperfed <- predsfed$fit + (1.96*predsfed$se.fit) # lower bounds

c(predfdjo1, lowerdjo1, upperdjo1)
c(predffed, lowerfed, upperfed)
```

We see that the model predicts Roger Federer has a 79.37% chance of winning the match, while Novak Djokovic has a 92.52% chance of winning the match. This is mainly because Djokovic's rank points were almost double that of Federer's at the time. However, Federer won the match. In this case, we had an anomaly. 

The next match we will look at was played at the China Open in 2013 and was between Novak Djokovic and Rafael Nadal.

```{r}
tennis %>%
  filter(name == "Novak Djokovic" | name == "Rafael Nadal",
         tourney_name == "Beijing",
         loser_name == "Rafael Nadal",
         tourney_id == "2013-747")

```

```{r}
djo2 <- data.frame(minutes = 87, ht = 188, rankpoints = 11120, ace = 5, df = 1)
nadal <- data.frame(minutes = 87, ht = 185, rankpoints = 10860, ace = 2, df = 2)
predict(final, newdata=djo2, type="response")
predict(final, newdata=nadal, type="response")
```

We see that the model predicts Novak Djokovic has a 91.82% chance of winning the match, while Rafael Nadal has a 87.96% chance of winning the match. Djokovic won the match. 

### Test Case with Similar Rank Points 

Now, we'll look at a match where both players had similar rank points. The match we will look at is between Jo Wilfried Tsonga and Michael Llodra, and was played at the Queen's Club Championships in 2011. 

```{r}
ten %>%
  filter(winner_rank_points > 1000 & winner_rank_points < 2000 & loser_rank_points > 1000 & loser_rank_points < 2000,
         tourney_id == "2011-311",
         tourney_name == "Queen's Club",
         winner_name == "Jo Wilfried Tsonga"
         )
```

```{r}
jo <- data.frame(minutes = 23, ht = 188, rankpoints = 1480, ace = 2, df = 1)
llodra <- data.frame(minutes = 23, ht = 190, rankpoints = 1400, ace = 0, df = 0)
predict(final, newdata=jo, type="response")
predict(final, newdata=llodra, type="response")
```

```{r}
predsjo <- predict(final, jo, type="response", se.fit=TRUE)
predsllodra <- predict(final, llodra, type="response", se.fit=TRUE)

predfjo <- predsjo$fit # predicted
lowerjo <- predsjo$fit - (1.96*predsjo$se.fit) # lower bounds
upperjo <- predsjo$fit + (1.96*predsjo$se.fit) # upper bounds

predfllodra <- predsllodra$fit # predicted
lowerllodra <- predsllodra$fit - (1.96*predsllodra$se.fit) # lower bounds
upperllodra <- predsllodra$fit + (1.96*predsllodra$se.fit) # lower bounds

c(predfjo, lowerjo, upperjo)
c(predfllodra, lowerllodra, upperllodra)
```


We see that the model predicts Jo Wilfried Tsonga has a 71.81% of chance of winning the match, while Michael Llodra has a 70.16% chance of winning the match. These percentages are lower because the players' rankpoints are significantly lower than those of Nadal's and Djokovic's. 

# Conclusion








